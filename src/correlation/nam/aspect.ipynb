{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import calendar\n",
    "import time\n",
    "from matplotlib import colors\n",
    "from sklearn import preprocessing\n",
    "import cartopy.crs as crs\n",
    "import cartopy.feature as cfeature\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(init):\n",
    "    years = [\"2018\", \"2019\", \"2020\", \"2021\"]\n",
    "    savedir = \"/home/aevans/ai2es/processed_data/frcst_err/\"\n",
    "\n",
    "    nam_fcast_and_error = []\n",
    "    gfs_fcast_and_error = []\n",
    "    hrrr_fcast_and_error = []\n",
    "\n",
    "    for year in years:\n",
    "        nam_fcast_and_error.append(\n",
    "            pd.read_parquet(\n",
    "                f\"{savedir}nam_fcast_and_error_df_{init}z_{year}_mask_water_ny.parquet\"\n",
    "            )\n",
    "        )\n",
    "        gfs_fcast_and_error.append(\n",
    "            pd.read_parquet(\n",
    "                f\"{savedir}gfs_fcast_and_error_df_{init}z_{year}_mask_water_ny.parquet\"\n",
    "            )\n",
    "        )\n",
    "        hrrr_fcast_and_error.append(\n",
    "            pd.read_parquet(\n",
    "                f\"{savedir}hrrr_fcast_and_error_df_{init}z_{year}_mask_water_ny.parquet\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "    nam_fcast_and_error_df = pd.concat(nam_fcast_and_error)\n",
    "    gfs_fcast_and_error_df = pd.concat(gfs_fcast_and_error)\n",
    "    hrrr_fcast_and_error_df = pd.concat(hrrr_fcast_and_error)\n",
    "\n",
    "    # need to remove the random forecasts that have forecast hours 0\n",
    "    # these are random because they only exist in the files that Ryan T. provided\n",
    "    gfs_fcast_and_error_df = gfs_fcast_and_error_df[\n",
    "        gfs_fcast_and_error_df[\"lead_time_ONLY_HOURS\"] != 0.0\n",
    "    ]\n",
    "    nam_fcast_and_error_df = nam_fcast_and_error_df[\n",
    "        nam_fcast_and_error_df[\"lead_time_ONLY_HOURS\"] != 0.0\n",
    "    ]\n",
    "    hrrr_fcast_and_error_df = hrrr_fcast_and_error_df[\n",
    "        hrrr_fcast_and_error_df[\"lead_time_ONLY_HOURS\"] != 0.0\n",
    "    ]\n",
    "    return gfs_fcast_and_error_df, nam_fcast_and_error_df, hrrr_fcast_and_error_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_df(df):\n",
    "    df = df[df['lead_time_HOUR'] <= 18]\n",
    "    error_months = (\n",
    "            df.groupby([df.time.dt.month, \"station\"])[\n",
    "                f\"t2m_error\"\n",
    "            ]\n",
    "            .mean()\n",
    "        ).to_frame().reset_index()\n",
    "    return error_months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corrs(df, lulc, keys):\n",
    "    df_pers = pd.DataFrame()\n",
    "    df_rho = pd.DataFrame()\n",
    "    df_tau = pd.DataFrame()\n",
    "    df_p_score = pd.DataFrame()\n",
    "    for i in np.arange(1,13):\n",
    "        pers_ls = []\n",
    "        rho_ls = []\n",
    "        tau_ls = []\n",
    "        p_score_ls = []\n",
    "        df = months_df[months_df['time'] == i]\n",
    "        for col, val in lulc.iteritems():\n",
    "            # get correlations\n",
    "            pers = scipy.stats.pearsonr(lulc[col], df['t2m_error'])[0]\n",
    "            p_score = scipy.stats.pearsonr(lulc[col], df['t2m_error'])[1]\n",
    "            rho = scipy.stats.spearmanr(lulc[col], df['t2m_error'])[0]\n",
    "            tau = scipy.stats.kendalltau(lulc[col], df['t2m_error'])[0]\n",
    "\n",
    "            # append\n",
    "            pers_ls.append(pers)\n",
    "            rho_ls.append(rho)\n",
    "            tau_ls.append(tau)\n",
    "            p_score_ls.append(p_score)\n",
    "\n",
    "        df_pers1 = pd.DataFrame(index=keys)\n",
    "        df_pers1[f'{i}'] = pers_ls\n",
    "        df_rho1 = pd.DataFrame(index=keys)\n",
    "        df_rho1[f'{i}'] = rho_ls\n",
    "        df_tau1 = pd.DataFrame(index=keys)\n",
    "        df_tau1[f'{i}'] = tau_ls\n",
    "        df_p_score1 = pd.DataFrame(index=keys)\n",
    "        df_p_score1[f'{i}'] = p_score_ls\n",
    "\n",
    "        df_pers = pd.concat([df_pers, df_pers1], axis = 1)\n",
    "        df_rho = pd.concat([df_rho, df_rho1], axis = 1)\n",
    "        df_tau = pd.concat([df_tau, df_tau1], axis = 1)\n",
    "        df_p_score = pd.concat([df_p_score, df_p_score1], axis=1)\n",
    "        \n",
    "    return df_pers, df_rho, df_tau, df_p_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_heatmap(df, corr_type):\n",
    "    fig, ax = plt.subplots(figsize=(12,9))\n",
    "    ax.set_title(f'{corr_type} Correlation Coefficients by Aspect/Slope')\n",
    "    ax = sns.heatmap(df.T, vmin = -1, vmax = 1, cmap=cm.seismic, annot = True)\n",
    "    ax.set_ylabel('Month')\n",
    "    ax.set_xlabel('Aspect/Slope')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = \"12\"\n",
    "\n",
    "gfs_fcast_and_error_df, nam_fcast_and_error_df, hrrr_fcast_and_error_df = read_data(\n",
    "    init\n",
    ")\n",
    "gfs_fcast_and_error_df = gfs_fcast_and_error_df.reset_index()\n",
    "nam_fcast_and_error_df = nam_fcast_and_error_df.reset_index()\n",
    "hrrr_fcast_and_error_df = hrrr_fcast_and_error_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lulc = pd.read_csv('/home/aevans/nwp_bias/src/correlation/data/aspect_nam.csv')\n",
    "lulc = lulc.drop(columns=['site', 'station'])\n",
    "keys = lulc.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "months_df = format_df(nam_fcast_and_error_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "months_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pers, df_rho, df_tau, df_p_score = get_corrs(months_df, lulc, keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_heatmap(df_pers, 'PERS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_heatmap(df_rho, 'RHO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_heatmap(df_tau, 'TAU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_heatmap(df_p_score, 'p_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.16 ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "44818f36aeaf89db1a1d21a2bee6031a28b4e41595a65903b38b9b0c4417365f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
